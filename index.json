[{"authors":["admin"],"categories":null,"content":"A happy geek: I love learning new skills and enjoy building large systems, almost just for the sake of it. Software Engineer with experience in all levels of projects, including design and architecture, development and test, and the setup of reliable production. Skilled at writing well designed low-level system programs using best practices in Go, C, C++. Fast learner, hard worker, and team player with flexibility using various tools. Dedicated to streamlining processes and efficiently resolving project issues in hand using the most adapted technology.\nAfter graduating from Télécom ParisTech in 2016, I worked three years in Korea as a Software Security Researcher. This gave me the opportunity to develop my ability to understand and solve abstract problems. I work one year on a project enhancing the Linux Kernel security before pivoting to fuzzing. I always pushed for my project to have high quality prototypes. Recently, one of my project was merged into the LLVM project, one of the largest C++ code bases, and heavily relied on by Google. At the end of 2019 I returned to France and worked at PacketAI, a young startup, to build a cloud monitoring platform: I was in charge of the agent producing data, and all the microservices treating it before handing either the UI or the ML services.\n \u0026ldquo;As the sun does not wait for prayers and incantations to be induced to rise, but immediately shines and is saluted by all, so do you also not wait for clappings of hands and shouts praise to be induced to do good, but be a doer of good voluntarily and you will be beloved as much as the sun.\u0026rdquo; Epictetus\n ","date":1590451200,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":1590451200,"objectID":"2234cd8eaccf5b320066c300624fca5f","permalink":"/authors/valentin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/valentin/","section":"authors","summary":"A happy geek: I love learning new skills and enjoy building large systems, almost just for the sake of it. Software Engineer with experience in all levels of projects, including design and architecture, development and test, and the setup of reliable production. Skilled at writing well designed low-level system programs using best practices in Go, C, C++. Fast learner, hard worker, and team player with flexibility using various tools. Dedicated to streamlining processes and efficiently resolving project issues in hand using the most adapted technology.","tags":null,"title":"","type":"authors"},{"authors":null,"categories":null,"content":"Quick Summary We all want to predict the future. Once we know what will happen, we can prepare and take advantage of the situation and become stronger. For example, if you know which stock will go up/down, you would make a lot of money. The ability to predict is the foundation of science: experiment to find models that can forecast the future. Nassim Taleb destroyed this idea in his book \u0026ldquo;The Black Swan\u0026rdquo;: The most impactful events, the game changers, are unpredictable. Although we often use post-rationalization to make it look like we could have foreseen the events after the fact. For example, the 9/11 attack.\nIn \u0026ldquo;Antifragile: Things That Gain from Disorder\u0026rdquo;, Taleb explains what we can do when we cannot predict the future but know it will change everything big time. The answer is in the title: we should build antifragile system. We know there will be shocks. Instead of predicting these shocks themselves and lower their consequences, we make systems that take advantage of them! Systems that get stronger thanks to the shocks.\nPersonal Impact Taleb directly speaks to me. While working doing research on Ankou and Entropic, I often confronted with this idea of how to deal with extremely rare events. In 2019, I got passionate about \u0026ldquo;Complex System\u0026rdquo; theory: system with so many inner interactions that their behavior cannot be well understood. I tried to read as much as possible on the subject: Geoffrey West, John Holland, Stuart Kauffman, and some others. I tried to understand \u0026ldquo;what can we do in this situation\u0026rdquo;.\nBasically, we cannot use the usual scientific approach of decomposing systems and then scaling up this understanding to predict the behavior of the whole system. Complex systems can only be characterized with high level statistical descriptors like entropy. We are limited to description, we cannot predict. And Taleb\u0026rsquo;s insight is a similar one: Complex systems, due to their many interactions have hidden, unforeseeable and non-linear behaviors. Their causal opacity and non-linearity make them perfect Black Swan generators. And guess what, almost everything we deal with nowadays is complex systems, are socio-economical system, most of the product we engineer (that\u0026rsquo;s why we can\u0026rsquo;t seem to solve all bugs), and of course nature itself.\nSolution We should stop trying to be smart. Stop trying to predict how systems are going to react to this or that shock. We should go back to our real goal: the payoff. Focus back on what we get in the end. If we put ourselves in an antifragile situation, we can benefit from future shocks, but we do not need to plan for them.\nThe Importance of Options  \u0026ldquo;Optionality is a substitute for intelligence.\u0026rdquo;\n  \u0026ldquo;The option is an agent of antifragility.\u0026rdquo;\n So how do we make ourselves antifragile? We give ourselves options. We often try to cut all which seems unnecessary, or redundant, to perform better at some given metric. But maybe this metric won\u0026rsquo;t be so important tomorrow. By having options, you give yourself more paths to explore in the future when you\u0026rsquo;ll get more information.\nTaleb likes to use the \u0026ldquo;rational flâneur\u0026rdquo; metaphor: when you travel, instead of having a two week plan of how you\u0026rsquo;ll visit this city or this country, you just plan a few days forward. This way you have the freedom to plan the rest of the trip when you acquire more information while you are already there. You don\u0026rsquo;t know what kind of information you will acquire, you don\u0026rsquo;t know what unexpected event can occur. By keeping a maximum of options available, you make sure that whatever happens, you\u0026rsquo;ll be able to transform to your benefit.\nThis \u0026ldquo;option\u0026rdquo; solution can be reformulated in a \u0026lsquo;Trial and error\u0026rsquo; policy. All the things you can try are your options. Notice that you still need some rationality to try options that have some potential, and then recognize a successful trial. Taleb is saying that we rely too much on models and top-down logic, but not that science and formalization are never useful. We still need some. Just much less of it.\nAnd in the end, this is exactly the evolutionary heuristics we use for fuzzers! Each seed is an option the fuzzer has. The fitness function is the way we chose the potential \u0026ldquo;good options\u0026rdquo; for the future.\nVia Negativa  \u0026ldquo;Rule No. 1: Never lose money. Rule No. 2: Don’t forget rule No. 1\u0026rdquo; Warren Buffet\n The other way Taleb suggests to achieve antifragility is to \u0026ldquo;remove the bad stuff\u0026rdquo;. It is not easy to predict what is going to happen, and it may be even harder to take advantage of it. However, it is often quite simple to detect fragility. The via negativa way is: detect the bad and remove it. This you are left with either robust, meaning that it does not have much reaction to shocks, negatively or positively, or antifragile.\n \u0026ldquo;We know a lot more about what is wrong than what is right. Negative knowledge is more robust to error than positive knowledge.\u0026rdquo;\n This is what he calls the \u0026ldquo;barbell\u0026rdquo; strategy. You have the robust part which you can\u0026rsquo;t lose; it is your insurance. And you have the antifragile part, which is riskier, but gives you exposure to different kinds of shocks by being diversified, and makes your profit/payoff. This is not very sophisticated, but in the end, less is more. This technique was lightly sketched out in \u0026ldquo;The Black Swan\u0026rdquo;, but without all of the fragility framework, so it was hard to understand why it would work.\nConclusion Do not be a fool. Do not over-optimize your system thinking you got everything under control. With a thin margin of safety, you risk losing it all. You are never fully safe from a Black Swan unless you set your system up this way.\n","date":1595116800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1595116800,"objectID":"d555a84c5e8ccdd48885f91b044dc389","permalink":"/post/antifragile/","publishdate":"2020-07-19T00:00:00Z","relpermalink":"/post/antifragile/","section":"post","summary":" We all want to **predict** the future. Once we know what will happen, we can prepare and take advantage of the situation and become stronger. For example, if you know which stock will go up/down, you would make a lot of money. The ability to predict is the foundation of science: experiment to find *models* that can forecast the future. Nassim Taleb destroyed this idea in his book 'The Black Swan': The most impactful events, the game changers, are unpredictable. ","tags":null,"title":"Antifragile: Why AI fails and alternative","type":"post"},{"authors":["Marcel Böhme","","Sang Kil Cha"],"categories":null,"content":"In this paper, we take the fundamental perspective of fuzzing as a learning process. Suppose before fuzzing, we know nothing about the behaviors of a program P: What does it do? Executing the first test input, we learn how P behaves for this input. Executing the next input, we either observe the same or discover a new behavior. As such, each execution reveals “some amount” of information about P’s behaviors. A classic measure of information is Shannon’s entropy. Measuring entropy allows us to quantify how much is learned from each generated test input about the behaviors of the program. Within a probabilistic model of fuzzing, we show how entropy also measures fuzzer efficiency. Specifically, it measures the general rate at which the fuzzer discovers new behaviors. Intuitively, efficient fuzzers maximize information.\nFrom this information theoretic perspective, we develop En- tropic, an entropy-based power schedule for greybox fuzzing which assigns more energy to seeds that maximize information. We implemented Entropic into the popular greybox fuzzer LibFuzzer. Our experiments with more than 250 open-source programs (60 million LoC) demonstrate a substantially improved efficiency and confirm our hypothesis that an efficient fuzzer maximizes informa- tion. Entropic has been independently evaluated and invited for integration into main-line LibFuzzer. Entropic will run on more than 25,000 machines fuzzing hundreds of security-critical software systems simultaneously and continuously.\nbody{ /* Normal */ font-size: 13pt; }  ","date":1590451200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1590451200,"objectID":"b40b6e33ce10db88de2f67684e1dd58b","permalink":"/publication/entropic/","publishdate":"2020-05-26T00:00:00Z","relpermalink":"/publication/entropic/","section":"publication","summary":"Entropic is an information-theoretic power schedule implemented based on LibFuzzer. It boosts performance by changing weights assigned to the seeds in the corpus. Seeds revealing more \"information\" are assigned a higher weight. Entropic has been independently evaluated by a team at Google and invited for integration into mainline LibFuzzer @ LLVM (C++ code base), whereupon Entropic was subject to a substantial code reviewing process.","tags":null,"title":"Boosting Fuzzer Efficiency: An Information Theoretic Perspective","type":"publication"},{"authors":["","Soomin Kim","Sang Kil Cha"],"categories":null,"content":"body{ /* Normal */ font-size: 13pt; }  ","date":1580860800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1580860800,"objectID":"3b74209e2270b39e5cbbc56a126644f2","permalink":"/publication/ankou/","publishdate":"2020-02-05T00:00:00Z","relpermalink":"/publication/ankou/","section":"publication","summary":"Grey-box fuzzing is an evolutionary process, which maintains and evolves a population of test cases with the help of a fitness function. Fitness functions used by current grey-box fuzzers are not informative in that they cannot distinguish different program executions as long as those executions achieve the same coverage. The problem is that current fitness functions only consider a union of data, but not their combination. As such, fuzzers often get stuck in a local optimum during their search. In this paper, we introduce Ankou, the first grey-box fuzzer that recognizes different _combinations_ of execution information, and present several scalability challenges encountered while designing and implementing Ankou. Our experimental results show that Ankou is 1.94× and 8.0× more effective in finding bugs than AFL and Angora, respectively.","tags":null,"title":"Ankou: Guiding Grey-box Fuzzing towards Combinatorial Difference","type":"publication"},{"authors":["","HyungSeok Chan","Choongwoo Han","Sang Kil Cha","Manuel Egele","Edward J. Schwartz","Maverick Woo"],"categories":null,"content":"Among the many software testing techniques available today, fuzzing has remained highly popular due to its conceptual simplicity, its low barrier to deployment, and its vast amount of empirical evidence in discovering real-world software vulnerabilities. At a high level, fuzzing refers to a process of repeatedly running a program with generated inputs that may be syntactically or semantically malformed.\n Our survey shows the fuzzing community is extremely vibrant. The recent surge of work by researchers and practitioners alike has made it difficult to gain a comprehensive and coherent view of fuzzing. Thus, it is easy to lose track of the design decisions and potentially important tweaks performed in each tool and paper. Furthermore, there has been an observable fragmentation in the terminology used by various fuzzers. For example, test case \u0026ldquo;minimization\u0026rdquo; and \u0026ldquo;reduction\u0026rdquo; are often used interchangeably. Such fragmentation makes it difficult to discover and disseminate knowledge and may severely hinder the progress in fuzzing research in the long run.\nTo help preserve and bring coherence to the vast literature of fuzzing, this paper presented a unified, general-purpose model of fuzzing together with a taxonomy of the current literature. Our terminology is chosen to closely reflect the current predominant usages, and our model is designed to suit a large number of fuzzing tasks. We surveyed academic papers from the major Security and Software Engineering conferences in the last 10 years, as well as projects having more than 100 stars on GitHub. The paper methodically explores the design decisions at every stage of the model by surveying the related literature and innovations that make modern-day fuzzers effective.\nCompanion Website Our contribution in this work is more than just the survey. Upon receiving the acceptance notice, we have started building a companion website. It is backed by a repository at GitHub, which contains the genealogy and the classification data of the surveyed fuzzers in the JSON format. We plan to keep this site up-to-date periodically through investing our own effort and accepting contributions from the community.\nbody{ /* Normal */ font-size: 13pt; }  ","date":1570665600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1570665600,"objectID":"c613964539060f72b7b5ca0b841b7b61","permalink":"/publication/survey/","publishdate":"2019-10-10T00:00:00Z","relpermalink":"/publication/survey/","section":"publication","summary":"This paper surveys both the academic papers and the open-sourced tools in the field of fuzzing. We present a unified, general-purpose model to better understand the design and trade-offs of fuzzers.","tags":null,"title":"The Art, Science, and Engineering of Fuzzing: A Survey","type":"publication"},{"authors":null,"categories":null,"content":"Hi everyone. This is my first \u0026lsquo;technical\u0026rsquo; blog post. I saw some people saying it helps growing your explanation skills which I sincerely lack. Thus, I decided next I struggle doing something because I feel it\u0026rsquo;s quite undocumented, I\u0026rsquo;ll try to make a post and explain how I did it. If even one person reads this and it\u0026rsquo;s even remotely useful to them, I\u0026rsquo;ll consider the job done. Ask any question, I\u0026rsquo;ll be happy to answer.\nNow down to the topic. I have trying to compile the kernel with clang ever since I have seen the LWN.net article on the topic. It says that you \u0026lsquo;just\u0026rsquo; need to go and compile your kernel with make CC=clang. So I went into the linux sources I had at this time (something around 4.9.60) and type the command. Obviously it didn\u0026rsquo;t work.\nTurns out, it\u0026rsquo;s not that much complicated. There are two requirements to compiling your kernel with clang:\n Get the right source and configuration. Get the right version of clang.  Everything is very well summarized on a LKML post from last November: https://lkml.org/lkml/2017/11/22/943.\nConcerning the source, I went for the simplest path: getting the latest stable version of the kernel with default configuration:\ngit clone git://git.kernel.org/pub/scm/linux/kernel/git/stable/linux-stable.git cd linux-stable git checkout v4.14.15 make defconfig  Here I\u0026rsquo;m on x86. If you want to compile for ARM64, it seems a bit more complicated. I didn\u0026rsquo;t try it but you might want to look for the android source that have not been upstreamed yet. Posting here because it\u0026rsquo;s not linked in the Matthias Kaehlcke\u0026rsquo;s LKML post:\n https://android.googlesource.com/kernel/common.git/+log/android-4.9 https://android.googlesource.com/kernel/common.git/+log/android-4.4  There also other documentation made by Google here which shows how to compile the kernel with KASAN and KCOV. Didn\u0026rsquo;t dig into that but could be useful to someone.\nIf you are looking for other configuration than just the default one, I didn\u0026rsquo;t try that and I think it\u0026rsquo;s really and a case by case basis. You are going to have to dig a little bit more.\nAnd for the version of clang, you need clang 5 or more. I on Debian Stretch for which LLVM has precompiled binary (see here if you are on another OS version). So add them to your source list:\nsudo vim /etc/apt/sources.list  And then add:\ndeb http://apt.llvm.org/stretch/ llvm-toolchain-stretch-6.0 main deb-src http://apt.llvm.org/stretch/ llvm-toolchain-stretch-6.0 main  Now you can install clang:\nsudo apt-get update sudo apt-get install clang-6.0  If you are not on Debian/Ubuntu, I am not sure whether or not there is another solution than just compiling LLVM directly (see here).\nFinally, we have the source and the compiler, so we are ready to compile. Go in your linux-stable/ folder and:\nmake CC=clang-6.0 -j4 bzImage  I just wanted the bzImage to boot, but you remove it if you want to compile everything. -j4 because gain time by making compilation parallel\u0026hellip; Now I don\u0026rsquo;t know how you plan to use your clang compiled kernel, but just to check it\u0026rsquo;s usable, I run it with qemu:\nqemu-system-x86_64 -kernel arch/x86/boot/bzImage -nographic -serial mon:stdio -append 'console=ttyS0'  And since we didn\u0026rsquo;t provide the -initrd option, it will crash at the end with something like:\n---[ end Kernel panic - not syncing: VFS: Unable to mount root fs on unknown-block(0,0)  There are plenty of guides on how to use your kernel, not the goal here.\nThat will conclude my first post. Hopefully it was clear enough. Don\u0026rsquo;t hesitate to contact me by any mean if you have any question or problems.\n","date":1515801600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1515801600,"objectID":"946c7d7e3e65e110170533c1396f626c","permalink":"/post/kernel_clang_compile/","publishdate":"2018-01-13T00:00:00Z","relpermalink":"/post/kernel_clang_compile/","section":"post","summary":"Hi everyone. This is my first \u0026lsquo;technical\u0026rsquo; blog post. I saw some people saying it helps growing your explanation skills which I sincerely lack. Thus, I decided next I struggle doing something because I feel it\u0026rsquo;s quite undocumented, I\u0026rsquo;ll try to make a post and explain how I did it. If even one person reads this and it\u0026rsquo;s even remotely useful to them, I\u0026rsquo;ll consider the job done. Ask any question, I\u0026rsquo;ll be happy to answer.","tags":null,"title":"Compiling Linux Kernel with Clang","type":"post"},{"authors":["","Daehee Jang","Chanho Ryu","Brent Byunghoon Kang"],"categories":null,"content":"body{ /* Normal */ font-size: 13pt; }  ","date":1514851200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1514851200,"objectID":"02344d0376c2c8b70cd1c8ecfe790ee3","permalink":"/publication/dikernel/","publishdate":"2018-01-02T00:00:00Z","relpermalink":"/publication/dikernel/","section":"publication","summary":"Monolithic kernel is one of the prevalent configurations out of various kernel design models. While monolithic kernel excels in performance and management, they are unequipped forruntime system update; and this brings the need for kernel extension. Although kernel extensions are a convenient measure for system management, it is well established that they make the system prone to rootkit attacks and kernel exploitation as they share the single memory space with the rest of the kernel. To address this problem, various forms of isolation (e.g., making into a process), are so far proposed, yet their performance overhead is often too high or incompatible for a general purpose kernel. In this paper, we propose Domain Isolated Kernel (DIKernel), a new kernel architecture which securely isolates the untrustedkernel extensions with minimal performance overhead. DIKernel leverages hardware-based memory domain feature in ARM architecture; and prevents system manipulation attacks originated from kernel extensions, such as rootkits and exploits caused by buggy kernel extensions. We implemented DIKernel on top of Linux 4.13 kernel with 1500 LOC.  Performance evaluation indicates that DIKernel imposes negligible overhead which is observed by cycle level microbenchmark.","tags":null,"title":"Domain Isolated Kernel: A lightweight sandbox for untrusted kernel extensions","type":"publication"}]